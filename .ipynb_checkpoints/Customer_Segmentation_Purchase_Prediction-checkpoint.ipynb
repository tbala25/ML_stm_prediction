{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "########## IMPORTS ####################\n",
    "from localLibrary_AWSConnector import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### get_data_s3 - returns a dictionary of dataframes from my s3 bucket\n",
    "###### <br> convert_time_to_int - converts time from YYYY-MM-DD HH:MM:SS format into an integer (YYYYMMDDHHMMSS)\n",
    "###### <br> calculate_time_diff - for each data source (SG,CRM,etc.) take Latest Date - Earliest Date to represent length of engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPER FUNCTIONS\n",
    "\n",
    "#Returns dictionary of dataframes\n",
    "def get_data_s3():\n",
    "    data_list = []\n",
    "    data_dict = {}\n",
    "\n",
    "    # Iterate through all buckets\n",
    "    for bucket in s3.buckets.all():\n",
    "        # Iterate through all items\n",
    "        for obj in s3.Bucket(bucket.name).objects.all():\n",
    "            if('data/' in obj.key and obj.key != 'data/'):\n",
    "                data_list.append(obj.key)\n",
    "                \n",
    "                # Save STM files as DataFrames\n",
    "                #if('STM' in obj.key and 'non' not in obj.key): \n",
    "                objct = s3.Bucket(bucket.name).Object(obj.key).get()\n",
    "                data_dict[obj.key] = pd.read_csv(objct['Body'], index_col=0)\n",
    "\n",
    "    print(data_list)\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "#Convert time helper\n",
    "def _convert_time_to_int(time):\n",
    "    #print(time)\n",
    "    if time != time:\n",
    "        return None\n",
    "    else:\n",
    "        return int(''.join(c for c in time if c.isdigit()))\n",
    "\n",
    "#convert date columns to int  for given df   \n",
    "def convert_time_int(df):\n",
    "    \n",
    "    df['EarliestCRM_int'] = [_convert_time_to_int(x) for x in df['EarliestCRM']]\n",
    "    df['LatestCRM_int'] = [_convert_time_to_int(x) for x in df['LatestCRM']]\n",
    "\n",
    "    df['LatestSeatGeek_int'] = [_convert_time_to_int(x) for x in df['LatestSeatGeekDate']]\n",
    "    df['EarliestSeatGeek_int'] = [_convert_time_to_int(x) for x in df['EarliestSeatGeekDate']]\n",
    "\n",
    "    df['EarliestMarketo_int'] = [_convert_time_to_int(x) for x in df['EarliestMarketoDate']]\n",
    "    df['LatestMarketo_int'] = [_convert_time_to_int(x) for x in df['LatestMarketoDate']]\n",
    "\n",
    "    df['EarliestFanatics_int'] = [_convert_time_to_int(x) for x in df['EarliestFanaticsDate']]\n",
    "    df['LatestFanatics_int'] = [_convert_time_to_int(x) for x in df['LatestFanaticsDate']]\n",
    "\n",
    "    df['EarliestYinzcam_int'] = [_convert_time_to_int(x) for x in df['EarliestYinzcamDate']]\n",
    "    df['LatestYinzcam_int'] = [_convert_time_to_int(x) for x in df['LatestYinzcamDate']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Creates date difference column (latest - earliest)\n",
    "def calculate_time_diff(df):\n",
    "    \n",
    "    df['CRM_diff'] = df['LatestCRM_int'] - df['EarliestCRM_int']\n",
    "    df['SeatGeek_diff'] = df['LatestSeatGeek_int'] - df['EarliestSeatGeek_int']\n",
    "    df['Marketo_diff'] = df['LatestMarketo_int'] - df['EarliestMarketo_int']\n",
    "    df['Fanatics_diff'] = df['LatestFanatics_int'] - df['EarliestFanatics_int']\n",
    "    df['Yinzcam_diff'] = df['LatestYinzcam_int'] - df['EarliestYinzcam_int']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = get_data_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all of the data we get a high level summary of each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in all_data.keys():\n",
    "    print(key)\n",
    "    print(all_data[key].describe(include='all'))\n",
    "    print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging all of the datasets together. Since the problem is posed as which single game ticket buyers or mini-plan buyers will convert to Season Ticket holders, we intiate dataframes with the SeatGeek (SG) dataset and then left merge (CRM, Yinzcam(YZ), Fanatics (FTS), Marketo (MKT))\n",
    " <br> With the SeatGeek data since the dataset is grouped by SSB_CRMSYSTEM_CONTACT_ID, Activity Type (Purchase, Sell, Transfer), and Secondary Ticket Type (Primary or Secondary/Resell) each fan may have multiple rows. So we have to pivot this so that each fan has one row and columns that represent the different activities across ticket type. Lastly we want to take the earliest engagement and latest engagement that fan has had within our SeatGeek data.\n",
    " <br><br> The below cell is commented out, the next one has more straightforward logic (loading SG first then the rest of the datasets)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MERGE all DF for STM\n",
    "# STM = None\n",
    "# nonSTM = None\n",
    "# lost = None\n",
    "\n",
    "\n",
    "# for key in all_data.keys():\n",
    "#     df = all_data[key]\n",
    "#     how = 'outer'\n",
    "#     if 'SG' in key:\n",
    "#         pivoted = pd.pivot_table(all_data[key], values=['TotalSeatGeekTransactions', 'TotalTicketVolume', 'TotalScannedTicketVolume', 'TotalTicketDollarValue'], index=['SSB_CRMSYSTEM_CONTACT_ID'],\n",
    "#                     columns=['cjsgActivityType', 'cjsgSecondaryTicketType'], aggfunc=np.sum)\n",
    "#         df = pd.DataFrame(pivoted.to_records())\n",
    "#         sg = pd.DataFrame()\n",
    "#         sg['SSB_CRMSYSTEM_CONTACT_ID'] = df['SSB_CRMSYSTEM_CONTACT_ID']\n",
    "#         sg['total_scanned'] = df[\"('TotalScannedTicketVolume', 'Purchase', 'Primary')\"] + df[\"('TotalScannedTicketVolume', 'Purchase', 'Resale')\"] + df[\"('TotalScannedTicketVolume', 'Purchase', 'Transfer')\"]\n",
    "\n",
    "#         sg['primary_purchase_transactions'] = df[\"('TotalSeatGeekTransactions', 'Purchase', 'Primary')\"]\n",
    "#         sg['secondary_purchase_transactions'] = df[\"('TotalSeatGeekTransactions', 'Purchase', 'Resale')\"] + df[\"('TotalSeatGeekTransactions', 'Purchase', 'Transfer')\"]\n",
    "#         sg['secondary_sell_transactions'] = df[\"('TotalSeatGeekTransactions', 'Sell', 'Resale')\"] + df[\"('TotalSeatGeekTransactions', 'Sell', 'Transfer')\"]\n",
    "\n",
    "#         sg['primary_purchase_dollars'] = df[\"('TotalTicketDollarValue', 'Purchase', 'Primary')\"]\n",
    "#         sg['secondary_purchase_dollars'] = df[\"('TotalTicketDollarValue', 'Purchase', 'Resale')\"] + df[\"('TotalTicketDollarValue', 'Purchase', 'Transfer')\"]\n",
    "#         sg['secondary_sell_dollars'] = df[\"('TotalTicketDollarValue', 'Sell', 'Resale')\"] + df[\"('TotalTicketDollarValue', 'Sell', 'Transfer')\"]\n",
    "\n",
    "#         sg['primary_purchase_tickets'] = df[\"('TotalTicketVolume', 'Purchase', 'Primary')\"]\n",
    "#         sg['secondary_purchase_tickets'] = df[\"('TotalTicketVolume', 'Purchase', 'Resale')\"] + df[\"('TotalTicketVolume', 'Purchase', 'Transfer')\"]\n",
    "#         sg['secondary_sell_tickets'] = df[\"('TotalTicketVolume', 'Sell', 'Resale')\"] + df[\"('TotalTicketVolume', 'Sell', 'Transfer')\"]\n",
    "        \n",
    "#         df = sg\n",
    "#         min_max_dates = all_data[key].groupby(['SSB_CRMSYSTEM_CONTACT_ID']).agg({'EarliestSeatGeekDate' : 'min','LatestSeatGeekDate' : 'max'})\n",
    "#         df = df.merge(min_max_dates[['EarliestSeatGeekDate' ,'LatestSeatGeekDate']], on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "#         how = 'right'\n",
    "        \n",
    "#     if('STM' in key and 'non' not in key): \n",
    "#         if STM is None:\n",
    "#             STM = df\n",
    "#         else:\n",
    "#             STM = STM.merge(df, how = how, on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "#     elif('non' in key): \n",
    "#         if nonSTM is None:\n",
    "#             nonSTM = df\n",
    "#         else:\n",
    "#             nonSTM = nonSTM.merge(df, how = how, on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "#     elif('lost' in key): \n",
    "#         if lost is None:\n",
    "#             lost = df\n",
    "#         else:\n",
    "#             lost = lost.merge(df, how = how, on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "#     else:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGE all DF for STM  #NEW USE SeatGeek and left join CRM, YZ, Marketo\n",
    "STM = None\n",
    "nonSTM = None\n",
    "lost = None\n",
    "\n",
    "\n",
    "for key in all_data.keys():\n",
    "    df = all_data[key]\n",
    "    if 'SG' in key:\n",
    "        pivoted = pd.pivot_table(all_data[key], values=['TotalSeatGeekTransactions', 'TotalTicketVolume', 'TotalScannedTicketVolume', 'TotalTicketDollarValue'], index=['SSB_CRMSYSTEM_CONTACT_ID'],\n",
    "                    columns=['cjsgActivityType', 'cjsgSecondaryTicketType'], aggfunc=np.sum)\n",
    "        df = pd.DataFrame(pivoted.to_records())\n",
    "        sg = pd.DataFrame()\n",
    "        sg['SSB_CRMSYSTEM_CONTACT_ID'] = df['SSB_CRMSYSTEM_CONTACT_ID']\n",
    "        sg['total_scanned'] = df[\"('TotalScannedTicketVolume', 'Purchase', 'Primary')\"] + df[\"('TotalScannedTicketVolume', 'Purchase', 'Resale')\"] + df[\"('TotalScannedTicketVolume', 'Purchase', 'Transfer')\"]\n",
    "\n",
    "        sg['primary_purchase_transactions'] = df[\"('TotalSeatGeekTransactions', 'Purchase', 'Primary')\"]\n",
    "        sg['secondary_purchase_transactions'] = df[\"('TotalSeatGeekTransactions', 'Purchase', 'Resale')\"] + df[\"('TotalSeatGeekTransactions', 'Purchase', 'Transfer')\"]\n",
    "        sg['secondary_sell_transactions'] = df[\"('TotalSeatGeekTransactions', 'Sell', 'Resale')\"] + df[\"('TotalSeatGeekTransactions', 'Sell', 'Transfer')\"]\n",
    "\n",
    "        sg['primary_purchase_dollars'] = df[\"('TotalTicketDollarValue', 'Purchase', 'Primary')\"]\n",
    "        sg['secondary_purchase_dollars'] = df[\"('TotalTicketDollarValue', 'Purchase', 'Resale')\"] + df[\"('TotalTicketDollarValue', 'Purchase', 'Transfer')\"]\n",
    "        sg['secondary_sell_dollars'] = df[\"('TotalTicketDollarValue', 'Sell', 'Resale')\"] + df[\"('TotalTicketDollarValue', 'Sell', 'Transfer')\"]\n",
    "\n",
    "        sg['primary_purchase_tickets'] = df[\"('TotalTicketVolume', 'Purchase', 'Primary')\"]\n",
    "        sg['secondary_purchase_tickets'] = df[\"('TotalTicketVolume', 'Purchase', 'Resale')\"] + df[\"('TotalTicketVolume', 'Purchase', 'Transfer')\"]\n",
    "        sg['secondary_sell_tickets'] = df[\"('TotalTicketVolume', 'Sell', 'Resale')\"] + df[\"('TotalTicketVolume', 'Sell', 'Transfer')\"]\n",
    "        \n",
    "        df = sg\n",
    "        min_max_dates = all_data[key].groupby(['SSB_CRMSYSTEM_CONTACT_ID']).agg({'EarliestSeatGeekDate' : 'min','LatestSeatGeekDate' : 'max'})\n",
    "        df = df.merge(min_max_dates[['EarliestSeatGeekDate' ,'LatestSeatGeekDate']], on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "        \n",
    "        if('STM' in key and 'non' not in key): \n",
    "            if STM is None:\n",
    "                STM = df\n",
    "\n",
    "        elif('non' in key): \n",
    "            if nonSTM is None:\n",
    "                nonSTM = df\n",
    "\n",
    "        elif('lost' in key): \n",
    "            if lost is None:\n",
    "                lost = df\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "for key in all_data.keys():\n",
    "    df = all_data[key]\n",
    "    if 'SG' not in key:\n",
    "        if('STM' in key and 'non' not in key): \n",
    "            STM = STM.merge(df, how = 'left', on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "\n",
    "        elif('non' in key): \n",
    "            nonSTM = nonSTM.merge(df, how = 'left', on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "\n",
    "        elif('lost' in key): \n",
    "            lost = lost.merge(df, how = 'left', on = 'SSB_CRMSYSTEM_CONTACT_ID')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Convert Date Columns and Calculate length of engagment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT DATE COLUMNS TO INT\n",
    "\n",
    "STM = convert_time_int(STM)\n",
    "nonSTM = convert_time_int(nonSTM)\n",
    "lost = convert_time_int(lost)\n",
    "\n",
    "#CALCULATE DATE DIFFERENCE\n",
    "#QUANTIFY LENGTH OF ENGAGEMENT\n",
    "\n",
    "STM = calculate_time_diff(STM)\n",
    "nonSTM = calculate_time_diff(nonSTM)\n",
    "lost = calculate_time_diff(lost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DROP DATE COLUMNS\n",
    "for col in STM.columns:\n",
    "    if 'Date' in col:\n",
    "        print(col)\n",
    "        STM.drop([col], axis=1, inplace = True)\n",
    "        nonSTM.drop([col], axis=1, inplace = True)\n",
    "        lost.drop([col], axis=1, inplace = True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "STM.drop(['EarliestCRM', 'LatestCRM'], axis=1, inplace = True)\n",
    "nonSTM.drop(['EarliestCRM', 'LatestCRM'], axis=1, inplace = True)\n",
    "lost.drop(['EarliestCRM', 'LatestCRM'], axis=1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"STM Length: {len(STM)}\")\n",
    "print(f\"nonSTM Length: {len(nonSTM)}\")\n",
    "print(f\"lost Length: {len(lost)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get percent null in each column across the merged datasets\n",
    "stm_null = pd.DataFrame(STM.isna().sum()/len(STM), columns = ['STM_Pct_Null'])\n",
    "nonstm_null = pd.DataFrame(nonSTM.isna().sum()/len(nonSTM), columns = ['nonSTM_Pct_Null'])\n",
    "lost_null = pd.DataFrame(lost.isna().sum()/len(lost), columns = ['lost_Pct_Null'])\n",
    "\n",
    "all_null = stm_null.merge(nonstm_null.merge(lost_null, left_index=True, right_index=True), left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_null.sort_values(by = 'STM_Pct_Null', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERCENT OF ROWS WITH MISSING VALUES\n",
    "print(f\"STM: {(STM.shape[0] - STM.dropna().shape[0])/len(STM)}\")\n",
    "print(f\"nonSTM: {(nonSTM.shape[0] - nonSTM.dropna().shape[0])/len(nonSTM)}\")\n",
    "print(f\"lost: {(lost.shape[0] - lost.dropna().shape[0])/len(lost)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP COLUMNS that had vast majority null\n",
    "columns = []\n",
    "for col in STM.columns:\n",
    "    if 'scanned' in col:\n",
    "        STM.drop([col], axis=1, inplace = True)\n",
    "        nonSTM.drop([col], axis=1, inplace = True)\n",
    "        lost.drop([col], axis=1, inplace = True)\n",
    "    elif 'secondary' in col:\n",
    "        STM.drop([col], axis=1, inplace = True)\n",
    "        nonSTM.drop([col], axis=1, inplace = True)\n",
    "        lost.drop([col], axis=1, inplace = True)\n",
    "    elif 'Yinz' in col:\n",
    "        STM.drop([col], axis=1, inplace = True)\n",
    "        nonSTM.drop([col], axis=1, inplace = True)\n",
    "        lost.drop([col], axis=1, inplace = True)\n",
    "    elif 'Fanatics' in col:\n",
    "        STM.drop([col], axis=1, inplace = True)\n",
    "        nonSTM.drop([col], axis=1, inplace = True)\n",
    "        lost.drop([col], axis=1, inplace = True)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "#NUMBER OF ROWS WITHOUT MISSING VALUES\n",
    "print(f\"STM full rows count: {(STM.dropna().shape[0])}\")\n",
    "print(f\"nonSTM full rows count: {(nonSTM.dropna().shape[0])}\")\n",
    "print(f\"lost full rows count: {(lost.dropna().shape[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = STM.hist(bins=30, grid=False, figsize=(20,15), color='#86bf91', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = nonSTM.hist(bins=30, grid=False, figsize=(20,15), color='#FFC733', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = lost.hist(bins=30, grid=False, figsize=(20,15), color='#FF5733', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STACK STM NONSTM, LOST\n",
    "\n",
    "STM['target'] = 'STM'\n",
    "lost['target'] = 'Rejecter'\n",
    "nonSTM['target'] = 'nonSTM'\n",
    "\n",
    "full_data = pd.concat([STM, nonSTM, lost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARDIZE FEATURES\n",
    "\n",
    "full_tmp = full_data.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(full_tmp)\n",
    "\n",
    "full_standardized  = pd.DataFrame(scaler.transform(full_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "\n",
    "#PCA CLUSTERING\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "#DECIDED ON # of Components = 5 to represent > 75% of variance\n",
    "pca = PCA()\n",
    "pca.fit(full_standardized)\n",
    "np.cumsum(pca.explained_variance_/sum(pca.explained_variance_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECIDED ON # of Components = 5 to represent > 75% of variance\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(full_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD ATTRIBUTES INFO FOR PCA INTERPRETATION\n",
    "# full_tmp.columns\n",
    "attributes_info = pd.DataFrame(columns = ['Attribute', 'Description'])\n",
    "attributes_info = attributes_info.append({'Attribute':0, 'Description': 'TotalCRMAcvtivty Volume'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':1, 'Description': 'TotalMarketoVolume'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':2, 'Description': 'primary_purchase_transactions'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':3, 'Description': 'primary_purchase_dollars'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':4, 'Description': 'primary_purchase_tickets'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':5, 'Description': 'EarliestCRM_int'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':6, 'Description': 'LatestCRM_int'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':7, 'Description': 'LatestSeatGeek_int'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':8, 'Description': 'EarliestSeatGeek_int'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':9, 'Description': 'EarliestMarketo_int'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':10, 'Description': 'LatestMarketo_int'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':11, 'Description': 'CRM_diff'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':12, 'Description': 'SeatGeek_diff'}, ignore_index = True)\n",
    "attributes_info = attributes_info.append({'Attribute':13, 'Description': 'Marketo_diff'}, ignore_index = True)\n",
    "attributes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "#display weights and direction of components\n",
    "def plot_feature_weights(df, pca, attributes_info, dimension, n_weights, plot=True, figsize=(5,10)):\n",
    "    \n",
    "    features = df.columns.values\n",
    "    components = pca.components_\n",
    "    feature_weights = dict(zip(features, components[dimension]))\n",
    "    sorted_weights = sorted(feature_weights.items(), key = lambda kv: kv[1])\n",
    "    \n",
    "    feat_names = []\n",
    "    feat_weights = []\n",
    "    feat_descs = []\n",
    "\n",
    "    for feature, weight in sorted_weights[-n_weights:]:\n",
    "        feat_names.append(feature)\n",
    "        feat_weights.append(weight)\n",
    "        \n",
    "    for feature, weight, in sorted_weights[:n_weights]:\n",
    "        feat_names.append(feature)\n",
    "        feat_weights.append(weight)\n",
    "        \n",
    "    for feature in feat_names:\n",
    "        if feature in attributes_info.Attribute.values:\n",
    "            feat_descs.append(attributes_info[attributes_info.Attribute == feature].Description.values[0])\n",
    "        else:\n",
    "            feat_descs.append(\"No description given\")\n",
    "    \n",
    "    component_info = {\"Feature\":feat_names, \"Description\":feat_descs, \"FeatureWeight\":feat_weights}\n",
    "    component_info = pd.DataFrame(component_info)\n",
    "    component_info.sort_values(by =[\"FeatureWeight\"], inplace=True, ascending=False)\n",
    "           \n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "\n",
    "        ax = fig.add_subplot(211)\n",
    "        ax.bar(feat_names, feat_weights)\n",
    "        ax.set_ylabel(\"Feature Weight\")\n",
    "        ax.set_xlabel(\"Feature Name\")\n",
    "        ax.set_title(\"PCA Feature weights - Component {}\".format(dimension))\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "        ax.grid()\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return component_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_0_info = plot_feature_weights(full_standardized, pca, attributes_info, 0, 3)\n",
    "component_0_info.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_1_info = plot_feature_weights(full_standardized, pca, attributes_info, 1, 3)\n",
    "component_1_info.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_2_info = plot_feature_weights(full_standardized, pca, attributes_info, 2, 3)\n",
    "component_2_info.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_3_info = plot_feature_weights(full_standardized, pca, attributes_info, 3, 3)\n",
    "component_3_info.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_4_info = plot_feature_weights(full_standardized, pca, attributes_info, 4, 3)\n",
    "component_4_info.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised: Clustering STM and Lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STM CLUSTERING\n",
    "\n",
    "df = STM.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna()\n",
    "stm_standardized  = pd.DataFrame(scaler.transform(df))\n",
    "stm_pca = pca.transform(stm_standardized)\n",
    "\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(2,10)\n",
    "for k in K:\n",
    "    km = cluster.KMeans(n_clusters=k)\n",
    "    km = km.fit(stm_pca)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    cluster_labels = km.labels_\n",
    "    silhouette_avg = silhouette_score(stm_pca, cluster_labels)\n",
    "    print(\"For n_clusters =\", k,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STM CLUSTERING - No real elbow in the char above so picked 5 clusters based on domain knowledge\n",
    "k = 5\n",
    "stm_kmeans = cluster.KMeans(n_clusters=k)\n",
    "stm_kmeans.fit(stm_pca)\n",
    "\n",
    "train_labels = stm_kmeans.predict(stm_pca)\n",
    "stm_pred  = pd.DataFrame(stm_pca)\n",
    "stm_pred['Cluster'] = train_labels\n",
    "stm_pred['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLY STM CLUSTERING to NONSTM\n",
    "df = nonSTM.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna()\n",
    "nonstm_standardized  = pd.DataFrame(scaler.transform(df))\n",
    "nonstm_pca = pca.transform(nonstm_standardized)\n",
    "\n",
    "train_labels = stm_kmeans.predict(nonstm_pca)\n",
    "nonstm_pred  = pd.DataFrame(nonstm_pca)\n",
    "nonstm_pred['Cluster_STM'] = train_labels\n",
    "nonstm_pred['Cluster_STM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOST CLUSTERING\n",
    "\n",
    "df = lost.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna()\n",
    "lost_standardized  = pd.DataFrame(scaler.transform(df))\n",
    "lost_pca = pca.transform(lost_standardized)\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(2,10)\n",
    "for k in K:\n",
    "    km = cluster.KMeans(n_clusters=k)\n",
    "    km = km.fit(lost_pca)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    cluster_labels = km.labels_\n",
    "    silhouette_avg = silhouette_score(lost_pca, cluster_labels)\n",
    "    print(\"For n_clusters =\", k,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOST CLUSTERING - Slight elbow at 6 but chose 5 based on domain knowledge ans consistency with STM clusters\n",
    "k = 5\n",
    "lost_kmeans = cluster.KMeans(n_clusters=k)\n",
    "lost_kmeans.fit(stm_pca)\n",
    "\n",
    "train_labels = lost_kmeans.predict(lost_pca)\n",
    "lost_pred  = pd.DataFrame(lost_pca)\n",
    "lost_pred['Cluster'] = train_labels\n",
    "lost_pred['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLY LOST CLUSTERING to NONSTM\n",
    "df = nonSTM.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna()\n",
    "nonstm_standardized  = pd.DataFrame(scaler.transform(df))\n",
    "nonstm_pca = pca.transform(nonstm_standardized)\n",
    "\n",
    "train_labels = lost_kmeans.predict(nonstm_pca)\n",
    "#nonstm_pred  = pd.DataFrame(nonstm_pca)\n",
    "nonstm_pred['Cluster_lost'] = train_labels\n",
    "nonstm_pred['Cluster_lost'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARE CLUSTER BREAKDOWNS\n",
    "\n",
    "cluster_info = pd.DataFrame([])\n",
    "\n",
    "cluster_info[\"STM\"] = stm_pred['Cluster'].value_counts().sort_index()\n",
    "cluster_info[\"Population_STM\"] = nonstm_pred['Cluster_STM'].value_counts().sort_index()\n",
    "cluster_info[\"Lost\"] = lost_pred['Cluster'].value_counts().sort_index()\n",
    "cluster_info[\"Population_Lost\"] = nonstm_pred['Cluster_lost'].value_counts().sort_index()\n",
    "cluster_info.reset_index(inplace=True)\n",
    "cluster_info.rename(columns={\"index\":\"Cluster\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE STM CLUSTER BREAKDOWNS\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 4))\n",
    "\n",
    "ax1.bar(cluster_info[\"Cluster\"], cluster_info[\"Population_STM\"], color=(255/255, 199/255, 51/255))\n",
    "ax1.set_xlabel(\"Cluster\")\n",
    "ax1.set_ylabel(\"No. of People\")\n",
    "ax1.set_title(\"General Population\")\n",
    "\n",
    "ax2.bar(cluster_info[\"Cluster\"], cluster_info[\"STM\"], color=(134/255, 191/255, 145/255))\n",
    "ax2.set_xlabel(\"Cluster\")\n",
    "ax2.set_ylabel(\"No. of People\")\n",
    "ax2.set_title(\"STM Customers\")\n",
    "\n",
    "fig.suptitle(\"Cluster Distributions\")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZE REJECTER/LOST CLUSTER BREAKDOWNS\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 4))\n",
    "\n",
    "ax1.bar(cluster_info[\"Cluster\"], cluster_info[\"Population_Lost\"], color=(255/255, 199/255, 51/255))\n",
    "ax1.set_xlabel(\"Cluster\")\n",
    "ax1.set_ylabel(\"No. of People\")\n",
    "ax1.set_title(\"General Population\")\n",
    "\n",
    "ax2.bar(cluster_info[\"Cluster\"], cluster_info[\"Lost\"], color=(255/255, 87/255, 51/255))\n",
    "ax2.set_xlabel(\"Cluster\")\n",
    "ax2.set_ylabel(\"No. of People\")\n",
    "ax2.set_title(\"Lost Customers\")\n",
    "\n",
    "fig.suptitle(\"Cluster Distributions\")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate cluster proportions\n",
    "\n",
    "cluster_info[\"Population_STM_proportion\"] = (cluster_info[\"Population_STM\"]/cluster_info[\"Population_STM\"].sum()*100).round(2)\n",
    "cluster_info[\"STM_proportion\"] = (cluster_info[\"STM\"]/cluster_info[\"STM\"].sum()*100).round(2)\n",
    "cluster_info[\"Population_Lost_proportion\"] = (cluster_info[\"Population_Lost\"]/cluster_info[\"Population_Lost\"].sum()*100).round(2)\n",
    "cluster_info[\"Lost_proportion\"] = (cluster_info[\"Lost\"]/cluster_info[\"Lost\"].sum()*100).round(2)\n",
    "\n",
    "cluster_info[\"STM_over_Pop\"] = cluster_info[\"STM_proportion\"] / cluster_info[\"Population_STM_proportion\"]\n",
    "cluster_info[\"Lost_over_Pop\"] = cluster_info[\"Lost_proportion\"] / cluster_info[\"Population_Lost_proportion\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Population proportions vs STM and Lost/Rejecter clusters \n",
    "\n",
    "fig, (ax, ax2) = plt.subplots(1,2, figsize=(10, 4))\n",
    "\n",
    "mask1 = cluster_info[\"STM_over_Pop\"] < 10\n",
    "mask2 = cluster_info[\"STM_over_Pop\"] >= 10\n",
    "\n",
    "ax.bar(cluster_info[\"Cluster\"][mask1], cluster_info[\"STM_over_Pop\"][mask1], color=(228/255, 235/255, 103/255))\n",
    "ax.bar(cluster_info[\"Cluster\"][mask2], cluster_info[\"STM_over_Pop\"][mask2], color=(134/255, 191/255, 145/255))\n",
    "\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Proportion Ratio\")\n",
    "ax.set_title(\"STM Population over General Population\")\n",
    "ax.axhline(y=1, linestyle = \"--\", linewidth = 0.8)\n",
    "\n",
    "\n",
    "mask1 = cluster_info[\"Lost_over_Pop\"] < 1\n",
    "mask2 = cluster_info[\"Lost_over_Pop\"] >= 1\n",
    "\n",
    "ax2.bar(cluster_info[\"Cluster\"][mask1], cluster_info[\"Lost_over_Pop\"][mask1], color=(255/255, 150/255, 51/255))\n",
    "ax2.bar(cluster_info[\"Cluster\"][mask2], cluster_info[\"Lost_over_Pop\"][mask2], color=(255/255, 87/255, 51/255))\n",
    "\n",
    "ax2.set_xlabel(\"Cluster\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax2.set_title(\"Lost Population over General Population\")\n",
    "ax2.axhline(y=1, linestyle = \"--\", linewidth = 0.8)\n",
    "\n",
    "\n",
    "fig.suptitle(\"Cluster Distributions\")\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_cluster(kmeans_model, cluster_no, data, pca_model, attributes_info, num_components=3, num_feat_per_comp=2):\n",
    "    \n",
    "    weights = kmeans_model.cluster_centers_[cluster_no]\n",
    "    components = list(range(len(weights)))\n",
    "    \n",
    "    cluster_expl = pd.DataFrame({\"Weights\":weights, \"Component\":components})\n",
    "    cluster_expl.sort_values(\"Weights\", ascending=False, inplace=True)\n",
    "\n",
    "    comps = []\n",
    "    weights = []\n",
    "    comp_infos = []\n",
    "    for index, row in cluster_expl.head(n=num_components).iterrows():\n",
    "        \n",
    "        component_info = plot_feature_weights(data, pca_model, attributes_info, \n",
    "                                                int(row[\"Component\"]), num_feat_per_comp, False)\n",
    "        comp_infos.append(component_info)\n",
    "        comps += [int(row[\"Component\"])] * len(component_info)\n",
    "        weights +=  [row[\"Weights\"]] * len(component_info)\n",
    "        \n",
    "    component_info = pd.concat(comp_infos, ignore_index=True)\n",
    "    component_info.insert(0, \"ComponentWeight\", pd.Series(weights))\n",
    "    component_info.insert(0, \"Component\", pd.Series(comps))\n",
    "        \n",
    "    return component_info.drop(['Description'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = explain_cluster(stm_kmeans, 0, nonSTM.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna(), pca, attributes_info)\n",
    "cluster_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = explain_cluster(stm_kmeans, 1, nonSTM.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna(), pca, attributes_info)\n",
    "cluster_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4 = explain_cluster(lost_kmeans, 4, nonSTM.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna(), pca, attributes_info)\n",
    "cluster_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3 = explain_cluster(lost_kmeans, 3, nonSTM.drop(['SSB_CRMSYSTEM_CONTACT_ID', 'target'], axis = 1).dropna(), pca, attributes_info)\n",
    "cluster_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised: STM or Rejecter/Lost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT full_data\n",
    "full_data = pd.concat([STM, nonSTM, lost])\n",
    "full_df = full_data.dropna()\n",
    "full_df = full_df.loc[full_df['target']!='nonSTM',]\n",
    "print(full_df['target'].value_counts())\n",
    "full_df['target'].value_counts()/len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    model = model.fit(X_train, y_train)\n",
    "    roc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    return roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = [(\"LogisticRegression\", LogisticRegression(random_state=42)),\n",
    "         (\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=42)),\n",
    "         (\"RandomForestClassifier\", RandomForestClassifier(random_state=42)),\n",
    "         (\"GradientBoostingClassifier\", GradientBoostingClassifier(random_state=42)),\n",
    "         (\"AdaBoostClassifier\", AdaBoostClassifier(random_state=42)),\n",
    "         (\"XGBClassifier\",xgb.XGBClassifier(random_state=42))]\n",
    "\n",
    "\n",
    "X = full_df.drop(['SSB_CRMSYSTEM_CONTACT_ID','target'], axis = 1)\n",
    "y = full_df['target']\n",
    "\n",
    "#HOLDOUT\n",
    "#X_use, X_holdout, y_use, y_holdout = train_test_split(X, y, stratify=y, test_size=0.1, random_state=42)\n",
    "X_use, X_holdout, y_use, y_holdout = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "#KFOLD\n",
    "# kf = StratifiedKFold(n_splits=5) # Define the split - into 5 folds \n",
    "# kf.get_n_splits(X_use, y_use) \n",
    "\n",
    "kf = KFold(n_splits=5) # Define the split - into 5 folds \n",
    "kf.get_n_splits(X_use) \n",
    "\n",
    "results = {\"Model\":[],\n",
    "          \"AUCROC_score\":[]}\n",
    "for name, model in models:\n",
    "    roc = 0\n",
    "#     for train_index, test_index in kf.split(X_use, y_use):\n",
    "    for train_index, test_index in kf.split(X_use):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_use.iloc[train_index], X_use.iloc[test_index]\n",
    "        y_train, y_test = y_use.iloc[train_index], y_use.iloc[test_index]\n",
    "        \n",
    "        r = train_and_predict(model, X_train, y_train, X_test, y_test)\n",
    "        roc += r\n",
    "    results[\"Model\"].append(name)\n",
    "    results[\"AUCROC_score\"].append(roc/5)\n",
    "    \n",
    "#SKLEARN MODELS + PYTORCH MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Holdout Class Proportion:\\n{y_holdout.value_counts()/len(y_holdout)}\\n\")\n",
    "print(f\"Train Class Proportion:\\n{y_train.value_counts()/len(y_train)}\\n\")\n",
    "print(f\"Test Class Proportion:\\n{y_test.value_counts()/len(y_test)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(results, orient='index').transpose()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN BEST MODEL\n",
    "model = xgb.XGBClassifier(random_state=42).fit(X_use, y_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ON HOLDOUT\n",
    "\n",
    "roc_score = roc_auc_score(y_holdout, model.predict_proba(X_holdout)[:,1])\n",
    "print(f\"Holdout ROC: {roc_score} \\n\")\n",
    "r = recall_score(y_holdout, model.predict(X_holdout), pos_label='STM')\n",
    "print(f\"Holdout Recall: {r} \\n\")\n",
    "p = precision_score(y_holdout, model.predict(X_holdout), pos_label='STM')\n",
    "print(f\"Precision Recall: {p} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ON HOLDOUT - -biased only STM\n",
    "\n",
    "#roc_score = roc_auc_score(y_holdout, model.predict_proba(X_holdout)[:,1])\n",
    "print(f\"Holdout ROC: No ROC for single class \\n\")\n",
    "r = recall_score(y_holdout[y_holdout=='STM'], model.predict(X_holdout[y_holdout=='STM']), pos_label='STM')\n",
    "print(f\"Holdout Recall: {r} \\n\")\n",
    "p = precision_score(y_holdout[y_holdout=='STM'], model.predict(X_holdout[y_holdout=='STM']), pos_label='STM')\n",
    "print(f\"Precision Recall: {p} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ON HOLDOUT - -biased only Rejecter\n",
    "\n",
    "#roc_score = roc_auc_score(y_holdout, model.predict_proba(X_holdout)[:,1])\n",
    "print(f\"Holdout ROC: No ROC for single class \\n\")\n",
    "r = recall_score(y_holdout[y_holdout=='Rejecter'], model.predict(X_holdout[y_holdout=='Rejecter']), pos_label='Rejecter')\n",
    "print(f\"Holdout Recall: {r} \\n\")\n",
    "p = precision_score(y_holdout[y_holdout=='Rejecter'], model.predict(X_holdout[y_holdout=='Rejecter']), pos_label='Rejecter')\n",
    "print(f\"Precision Recall: {p} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, model_name, feature_names, num_features=10):\n",
    "    \n",
    "    feature_importance_values= np.zeros((len(model.feature_importances_)))\n",
    "    \n",
    "    feature_importance_values += model.feature_importances_\n",
    "\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # sort based on importance\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "    # normalize the feature importances to add up to one\n",
    "    feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "    feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.barh(list(reversed(list(feature_importances.index[:num_features]))), \n",
    "                feature_importances['normalized_importance'][:num_features], \n",
    "                align = 'center')\n",
    "\n",
    "    # Set ticks and labels\n",
    "    ax.set_yticks(list(reversed(list(feature_importances.index[:num_features]))))\n",
    "    ax.set_yticklabels(feature_importances['feature'][:num_features])\n",
    "    ax.set_xlabel('Normalized Importance')\n",
    "    ax.set_title(f'Feature Importances ({model_name})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(model=model, model_name=\"XGBoost\", feature_names=X_use.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE FOR INTERPRETABILITY\n",
    "model = DecisionTreeClassifier(random_state=42, min_impurity_decrease = .001).fit(X_use, y_use)\n",
    "roc_score = roc_auc_score(y_holdout, model.predict_proba(X_holdout)[:,1])\n",
    "print(f\"Holdout ROC: {roc_score} \\n\")\n",
    "r = recall_score(y_holdout, model.predict(X_holdout), pos_label='STM')\n",
    "print(f\"Holdout Recall: {r} \\n\")\n",
    "p = precision_score(y_holdout, model.predict(X_holdout), pos_label='STM')\n",
    "print(f\"Precision Recall: {p} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT TREE\n",
    "plt.figure(figsize=(45,45))\n",
    "plt.rcParams.update({'font.size':22})\n",
    "tree.plot_tree(model, filled=True, feature_names = X_use.columns, class_names = ['STM', 'Rejecter'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on nonSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonSTMnew = nonSTM.drop(['SSB_CRMSYSTEM_CONTACT_ID','target'], axis = 1).dropna()\n",
    "pd.Series(model.predict(nonSTMnew)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonSTMnew[model.predict(nonSTMnew)=='STM']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
